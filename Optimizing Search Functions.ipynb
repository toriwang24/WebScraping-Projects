{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb34583e-fcd5-4a1e-81aa-c9bf9587f746",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0742744-81fd-4c45-996d-6574f772f7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# Webscraping Imports\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# ML library imports\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ngrams\n",
    "import nltk.collocations\n",
    "from nltk import BigramCollocationFinder\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Formatting\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d8dac5-2ee5-487e-afc6-210fb0f62502",
   "metadata": {},
   "source": [
    "# Defining Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a718c086-997e-4358-92d6-470e33f53049",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Webscraping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0785ab52-e4bc-483a-a9f1-7baf25044aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "page_limit = 200\n",
    "page_num = 1\n",
    "request_url = f\"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=Data%2BScientist&location=San%2BFrancisco%2BBay%2BArea&geoId=90000084&trk=public_jobs_jobs-search-bar_search-submit&start={page_num}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50b15639-4e47-49b0-b362-a2f603874744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_jobs(request_url, page_limit):\n",
    "    job_list = []\n",
    "    for page_num in range(1,page_limit):\n",
    "        list_url = request_url \n",
    "        # Getting response request from list \n",
    "        response = requests.get(list_url)\n",
    "    \n",
    "        list_data = response.text\n",
    "        list_soup = BeautifulSoup(list_data, 'html.parser')\n",
    "        page_jobs = list_soup.find_all(\"li\")\n",
    "        #10 jobs per page\n",
    "        #print(len(page_jobs))\n",
    "        \n",
    "        ## Get job ID's from each page\n",
    "        id_list = []\n",
    "    \n",
    "        for job in page_jobs:\n",
    "            base_card_div = job.find(\"div\", {\"class\": \"base-card\"})\n",
    "            job_id = base_card_div.get(\"data-entity-urn\").split(\":\")[3]\n",
    "            id_list.append(job_id)\n",
    "            \n",
    "        # For every job with ID, get the information\n",
    "        for job_id in id_list:\n",
    "            job_url = f\"https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/{job_id}\"\n",
    "            job_response = requests.get(job_url)\n",
    "            if(job_response.status_code == 200):\n",
    "                #print(job_response.status_code)\n",
    "                job_soup = BeautifulSoup(job_response.text, \"html.parser\")\n",
    "                job_post = {}\n",
    "                try:\n",
    "                    job_post[\"days_ago_posted\"] = job_soup.find( \"span\", {\"class\": \"posted-time-ago__text topcard__flavor--metadata\"}).text.strip()\n",
    "                except:\n",
    "                    job_post[\"days_ago_posted\"] = None\n",
    "                if(job_post[\"days_ago_posted\"] != None and (\"days\" in job_post[\"days_ago_posted\"] or \"hours\" in job_post[\"days_ago_posted\"])): \n",
    "                    try:\n",
    "                        job_post[\"company_name\"] = job_soup.find( \"a\", {\"class\": \"topcard__org-name-link topcard__flavor--black-link\"}).text.strip()\n",
    "                    except: \n",
    "                        job_post[\"company_name\"] = None\n",
    "                    try:\n",
    "                        job_post[\"job_title\"] = job_soup.find( \"h2\", {\"class\": \"top-card-layout__title font-sans text-lg papabear:text-xl font-bold leading-open text-color-text mb-0 topcard__title\"}).text.strip()\n",
    "                    except: \n",
    "                        job_post[\"job_title\"] = None\n",
    "                    try:\n",
    "                        job_post[\"job_description\"] = job_soup.find( \"div\", {\"class\":\"show-more-less-html__markup show-more-less-html__markup--clamp-after-5 relative overflow-hidden\" }).text.strip()\n",
    "                    except: \n",
    "                        job_post[\"job_description\"] = None\n",
    "                    try:\n",
    "                        job_post[\"num_applicants\"] = job_soup.find( \"figcaption\", {\"class\": \"num-applicants__caption\"}).text.strip()\n",
    "                    except:\n",
    "                        job_post[\"num_applicants\"] = None\n",
    "                    job_list.append(job_post)\n",
    "    return pd.DataFrame(job_list)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10702101-011b-47a2-a4d0-d8bd1b82b28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_df = scrape_jobs(request_url, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b2c2d89-79f7-4977-9f44-183d1df64b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>days_ago_posted</th>\n",
       "      <th>company_name</th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_description</th>\n",
       "      <th>num_applicants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3 days ago</td>\n",
       "      <td>Wealthfront</td>\n",
       "      <td>Data Scientist, Marketing</td>\n",
       "      <td>The Wealthfront Data Science team utilizes our...</td>\n",
       "      <td>Over 200 applicants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4 days ago</td>\n",
       "      <td>Notion</td>\n",
       "      <td>Data Scientist, Growth</td>\n",
       "      <td>About UsWe're on a mission to make it possible...</td>\n",
       "      <td>Over 200 applicants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5 days ago</td>\n",
       "      <td>Persistent Systems</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>About PersistentWe are a trusted Digital Engin...</td>\n",
       "      <td>Over 200 applicants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6 days ago</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>AI Engineer</td>\n",
       "      <td>LinkedIn is the world’s largest professional n...</td>\n",
       "      <td>Over 200 applicants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2 days ago</td>\n",
       "      <td>Brex</td>\n",
       "      <td>Data Scientist II, Credit</td>\n",
       "      <td>Why join usBrex is the AI-powered spend platfo...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  days_ago_posted        company_name                  job_title  \\\n",
       "0      3 days ago         Wealthfront  Data Scientist, Marketing   \n",
       "1      4 days ago              Notion     Data Scientist, Growth   \n",
       "2      5 days ago  Persistent Systems             Data Scientist   \n",
       "3      6 days ago            LinkedIn                AI Engineer   \n",
       "4      2 days ago                Brex  Data Scientist II, Credit   \n",
       "\n",
       "                                     job_description       num_applicants  \n",
       "0  The Wealthfront Data Science team utilizes our...  Over 200 applicants  \n",
       "1  About UsWe're on a mission to make it possible...  Over 200 applicants  \n",
       "2  About PersistentWe are a trusted Digital Engin...  Over 200 applicants  \n",
       "3  LinkedIn is the world’s largest professional n...  Over 200 applicants  \n",
       "4  Why join usBrex is the AI-powered spend platfo...                 None  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491a9694-bc5f-4e42-b6e0-ba19fcd344be",
   "metadata": {},
   "source": [
    "## ML Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e2c670b-5cae-4fa4-a854-b1d5ca8e0929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stop_words():\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    stop_words.add(':')\n",
    "    stop_words.add(',')\n",
    "    stop_words.add('.')\n",
    "    stop_words.add(', ')\n",
    "    stop_words.add('. ')\n",
    "    stop_words.add('*')\n",
    "    stop_words.add(\"'\")\n",
    "    stop_words.add(\"'s\")\n",
    "    stop_words.add('e.g')\n",
    "    stop_words.add('employees')\n",
    "    stop_words.add('applicants')\n",
    "    stop_words.add(')')\n",
    "    stop_words.add('(')\n",
    "    stop_words.add('eligible')\n",
    "    stop_words.add('participate')\n",
    "    return(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05bba00b-6a28-44f5-83a9-8b6d5e75ef87",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = create_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae5b9f4b-c113-4bcb-92db-050960bbfc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_grams(word_list, n, top_n):\n",
    "    '''\n",
    "        Function to return ngrams\n",
    "        word_list: pass lowercase word list (filtered for stop words)\n",
    "        n: the number of words in each phrase (gram)\n",
    "        top_n: top n number of matches \n",
    "    '''\n",
    "    grams = list(ngrams(word_list, n))\n",
    "    freq_dist = FreqDist(grams)\n",
    "    \n",
    "    topn = freq_dist.most_common(top_n)\n",
    "    return(topn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a472ef7-8e09-44a9-8e07-58ab9728ff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_n_grams():\n",
    "    bigram_list = []\n",
    "    # trigram_list = []\n",
    "    \n",
    "    #Create a dictionary of the most common bigrams\n",
    "    bigram_dict = {}\n",
    "    # trigram_dict = {}\n",
    "    \n",
    "    #for every job in job_list dataframe\n",
    "    for i in range(len(jobs_df)): # change to len(jobs_df)\n",
    "        # print(jobs_df['job_description'][i], '\\n')\n",
    "        # Grab the job description\n",
    "        text = jobs_df['job_description'][i]\n",
    "    \n",
    "        #Tokenize and filter unnecessary words, and put relevant words i\n",
    "        tokens = word_tokenize(text)\n",
    "        filtered_list = []\n",
    "        for word in tokens:\n",
    "            if word.casefold() not in stop_words:\n",
    "                filtered_list.append(word.lower())  \n",
    "        \n",
    "        # For bigrams\n",
    "        # add bigrams\n",
    "        top8 = get_n_grams(filtered_list, 2, 8)\n",
    "        bigram_list.append(top8)\n",
    "        for tup in top8:\n",
    "            if tup[0] not in bigram_dict:\n",
    "                bigram_dict[tup[0]] = tup[1]\n",
    "            else:\n",
    "                bigram_dict[tup[0]] += tup[1]\n",
    "    return (sorted(bigram_dict.items(), key=lambda item: item[1],  reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38e418a6-7320-4125-a795-633d5c1f8b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigrams_trigrams(jobs_df):\n",
    "\n",
    "    bigram_list = []\n",
    "    trigram_list = []\n",
    "    #Create a dictionary of the most common bigrams\n",
    "    bigram_dict = {}\n",
    "    trigram_dict = {}\n",
    "    \n",
    "    #for every job in job_list dataframe\n",
    "    for i in range(len(jobs_df)): # change to len(jobs_df)\n",
    "        # print(jobs_df['job_description'][i], '\\n')\n",
    "        # Grab the job description\n",
    "        text = jobs_df['job_description'][i]\n",
    "    \n",
    "        #Tokenize and filter unnecessary words, and put relevant words i\n",
    "        tokens = word_tokenize(text)\n",
    "        filtered_list = []\n",
    "        for word in tokens:\n",
    "            if word.casefold() not in stop_words:\n",
    "                filtered_list.append(word.lower())  \n",
    "        \n",
    "        # add bigrams\n",
    "        top8 = get_n_grams(filtered_list, 2, 8)\n",
    "        bigram_list.append(top8)\n",
    "        for tup in top8:\n",
    "            if tup[0] not in bigram_dict:\n",
    "                bigram_dict[tup[0]] = tup[1]\n",
    "            else:\n",
    "                bigram_dict[tup[0]] += tup[1]\n",
    "    \n",
    "        # add trigrams\n",
    "        top5 = get_n_grams(filtered_list, 3, 2)\n",
    "        bigram_list.append(top5)\n",
    "        for tup in top5:\n",
    "            if tup[0] not in bigram_dict:\n",
    "                trigram_dict[tup[0]] = tup[1]\n",
    "            else:\n",
    "                trigram_dict[tup[0]] += tup[1]\n",
    "    \n",
    "    sorted_bigram = (sorted(bigram_dict.items(), key=lambda item: item[1],  reverse=True))\n",
    "    sorted_trigram = (sorted(trigram_dict.items(), key=lambda item: item[1],  reverse=True))\n",
    "    return(sorted_bigram, sorted_trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1bafd273-bd34-4e47-a202-e154d3637aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "(sorted_bigram, sorted_trigram) = get_bigrams_trigrams(jobs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3bfbb740-0657-439f-af7b-a96204bddc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('credit', 'risk'), 8),\n",
      " (('data', 'science'), 6),\n",
      " (('machine', 'learning'), 6),\n",
      " (('wealthfront', 'brokerage'), 5),\n",
      " (('wealthfront', 'advisers'), 4),\n",
      " (('data', 'scientist'), 4),\n",
      " (('marketing', 'team'), 3),\n",
      " (('data', 'scientists'), 3),\n",
      " (('cash', 'account'), 3),\n",
      " (('1+', 'years'), 3),\n",
      " (('equal', 'opportunity'), 3),\n",
      " (('risk', 'management'), 3),\n",
      " (('computer', 'science'), 2),\n",
      " (('natural', 'sciences'), 2),\n",
      " (('statistical', 'inference'), 2),\n",
      " (('inference', 'experimentation'), 2),\n",
      " (('committed', 'providing'), 2),\n",
      " (('see', 'beyond'), 2),\n",
      " (('beyond', 'rise'), 2),\n",
      " (('nifty', 'midcap'), 2),\n",
      " (('ability', 'go'), 2),\n",
      " (('go', 'deep'), 2),\n",
      " (('deep', 'complex'), 2),\n",
      " (('complex', 'vague'), 2),\n",
      " (('role', 'based'), 2),\n",
      " (('based', 'sunnyvale'), 2),\n",
      " (('sunnyvale', 'san'), 2),\n",
      " (('san', 'francisco'), 2),\n",
      " (('francisco', 'bellevue'), 2),\n",
      " (('using', 'data'), 2),\n",
      " (('closely', 'credit'), 2),\n",
      " (('credit', 'enterprise'), 2),\n",
      " (('enterprise', 'risk'), 2),\n",
      " (('analytical', 'frameworks'), 2),\n",
      " (('uswe', \"'re\"), 1),\n",
      " ((\"'re\", 'mission'), 1),\n",
      " (('mission', 'make'), 1),\n",
      " (('make', 'possible'), 1)]\n"
     ]
    }
   ],
   "source": [
    "pprint(sorted_bigram[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89aae918-9c37-44a4-abbd-de5c07485243",
   "metadata": {},
   "source": [
    "# Business Analyst Positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6537cd15-353e-4bd0-8082-38a6761d298b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('solutions', 'inc.'), 183),\n",
      " (('regression', 'testing'), 122),\n",
      " (('business', 'analystw2'), 61),\n",
      " (('analystw2', 'contractsalary'), 61),\n",
      " (('contractsalary', 'range'), 61),\n",
      " (('range', '$'), 61),\n",
      " (('$', '83,200'), 61),\n",
      " (('83,200', '-'), 61),\n",
      " (('computer', 'science/statistics'), 56),\n",
      " (('sunnyvale', 'ca-hybridmust'), 28),\n",
      " (('ca-hybridmust', 'skills-need'), 28),\n",
      " (('skills-need', 'candidates'), 28),\n",
      " (('candidates', 'strong'), 28),\n",
      " (('strong', 'sql/python'), 28),\n",
      " (('sql/python', 'background'), 28),\n",
      " (('background', 'well'), 28),\n",
      " (('marketing', 'strategy'), 10),\n",
      " (('product', 'marketing'), 8),\n",
      " (('b2b', 'marketing'), 6),\n",
      " (('content', 'strategy'), 6),\n",
      " (('recruiting', 'hiring'), 6),\n",
      " (('chemistry', 'cheminformatics'), 5),\n",
      " (('online', 'travel'), 4),\n",
      " (('strategy', 'analyst'), 4),\n",
      " (('analyst', 'b2b'), 4),\n",
      " (('b2b', 'content'), 4),\n",
      " (('small', 'molecule'), 4),\n",
      " (('molecule', 'design'), 4),\n",
      " (('years', 'chemistry'), 4),\n",
      " (('cheminformatics', 'life'), 4),\n",
      " (('life', 'sciences'), 4),\n",
      " (('sciences', 'chemical'), 4),\n",
      " (('chemical', 'engineering'), 4),\n",
      " (('growth', 'strategy'), 4),\n",
      " (('sales', 'leadership'), 4),\n",
      " (('market', 'segmentation'), 4),\n",
      " (('key', 'performance'), 4),\n",
      " (('fair', 'chance'), 4),\n",
      " (('nelson', 'connects'), 3),\n",
      " (('get', 'best'), 2),\n",
      " (('best', 'candidate'), 2),\n",
      " (('sales', 'analyst'), 2),\n",
      " (('pricing', 'files'), 2),\n",
      " (('analyst', '-'), 1),\n",
      " (('-', 'napafor'), 1),\n",
      " (('napafor', '50'), 1),\n",
      " (('50', 'years'), 1),\n",
      " (('years', 'nelson'), 1)]\n"
     ]
    }
   ],
   "source": [
    "page_limit = 200\n",
    "page_num = 1\n",
    "request_url = f\"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=Business%2BAnalyst&location=San%2BFrancisco%2BBay%2BArea&geoId=90000084&trk=public_jobs_jobs-search-bar_search-submit&start={page_num}\"\n",
    "\n",
    "jobs_df = scrape_jobs(request_url, 200)\n",
    "(sorted_bigram, sorted_trigram) = get_bigrams_trigrams(jobs_df)\n",
    "\n",
    "pprint(sorted_bigram[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ca6b02-4da3-4ab9-8517-f71eeacb2440",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
